{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Turbocaspal/AutoGPT/blob/master/Transkript_o_Vits_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fUTBK8pZvEL"
      },
      "source": [
        "# Transkript-o-Vits AssemblyAI\n",
        "Dieses Notebook ermöglicht die automatische Transkription und die Erstellung eines Meeting-Protokolls aus Audiodateien."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtKRng9hZvEU"
      },
      "source": [
        "## Installation der notwendigen Bibliotheken\n",
        "Im ersten Schritt werden alle notwendigen Bibliotheken wie AssemblyAI und OpenAI installiert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjNJLOdsZvEW"
      },
      "outputs": [],
      "source": [
        "!pip install -U assemblyai --quiet\n",
        "!pip install -U openai --quiet\n",
        "!pip install markdown --quiet\n",
        "!pip install tiktoken --quiet\n",
        "!pip install python-docx --quiet\n",
        "!pip install python-pptx --quiet\n",
        "!pip install exceptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u7oiC9MZvEa"
      },
      "source": [
        "## API-Keys einrichten\n",
        "Stelle sicher, dass die API-Keys in den Colab-Secrets hinterlegt sind:\n",
        "- `assemblyai`: AssemblyAI-API-Key\n",
        "- `OPENAI_API_KEY`: OpenAI-API-Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7ud71zKZvEc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import assemblyai as aai\n",
        "\n",
        "# AssemblyAI API-Key aus Secrets laden\n",
        "aai_key = userdata.get('assemblyai')\n",
        "if not aai_key:\n",
        "    raise ValueError(\"AssemblyAI-API-Key nicht gefunden. Bitte in den Secrets unter 'assemblyai' hinterlegen.\")\n",
        "aai.settings.api_key = aai_key\n",
        "\n",
        "# OpenAI API-Key aus Secrets laden\n",
        "openai_key = userdata.get('OPENAI_API_KEY')\n",
        "if not openai_key:\n",
        "    raise ValueError(\"OpenAI-API-Key nicht gefunden. Bitte in den Secrets unter 'OPENAI_API_KEY' hinterlegen.\")\n",
        "\n",
        "# Ionos API-Key aus Secrets laden\n",
        "openai_key = userdata.get('IONOS_API_TOKEN')\n",
        "if not openai_key:\n",
        "    raise ValueError(\"OpenAI-API-Key nicht gefunden. Bitte in den Secrets unter 'IONOS_API_TOKEN' hinterlegen.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aYyS_9nZvEe"
      },
      "source": [
        "## Datei auswählen oder hochladen\n",
        "Wähle, ob eine Datei hochgeladen werden soll, oder ob eine vorhandene Datei aus dem Verzeichnis `/content/` verwendet wird."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ_b4wuTZvEg"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from google.colab import files\n",
        "\n",
        "choice = input(\"Möchtest du eine Datei hochladen (1) oder eine vorhandene Datei aus /content/ verwenden (2)? (Standard: 1): \") or \"1\"\n",
        "\n",
        "if choice == \"1\":\n",
        "    print(\"Bitte lade eine MP3- oder WAV-Datei hoch:\")\n",
        "    uploaded = files.upload()\n",
        "    audio_file = next(iter(uploaded))\n",
        "elif choice == \"2\":\n",
        "    print(\"Durchsuche /content/ nach MP3- oder WAV-Dateien...\")\n",
        "    audio_file = None\n",
        "    for file in os.listdir(\"/content\"):\n",
        "        if file.endswith(\".mp3\") or file.endswith(\".wav\") or file.endswith(\".m4a\") or file.endswith(\".mp4\"):\n",
        "            audio_file = file\n",
        "            break\n",
        "    if not audio_file:\n",
        "        raise FileNotFoundError(\"Keine MP3- oder WAV-Datei im Ordner /content gefunden.\")\n",
        "    print(f\"Verwendete Datei: {audio_file}\")\n",
        "else:\n",
        "    raise ValueError(\"Ungültige Auswahl. Bitte wähle entweder 1 oder 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hF8BDpTGZvEi"
      },
      "source": [
        "## Transkription mit AssemblyAI\n",
        "Die ausgewählte Datei wird mit AssemblyAI transkribiert. Die Ergebnisse werden in einer Markdown-Datei gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqV6v5LpZvEj"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Optionen für die verbesserte Sprecherdiarisierung und Sentimentanalyse\n",
        "use_sentiment = input(\"Soll eine Sentimentanalyse durchgeführt werden? (ja/nein, Standard: nein): \") or \"nein\"\n",
        "use_sentiment = use_sentiment.lower() == \"ja\"\n",
        "\n",
        "# Anzahl Sprecher muss immer noch abgefragt werden\n",
        "speakers_expected = input(\"Anzahl Sprecher:\")\n",
        "\n",
        "language_code = \"de\"  # Sprache auf Deutsch setzen\n",
        "\n",
        "if language_code == \"de\" and use_sentiment:\n",
        "    print(\"Sentimentanalyse ist für Deutsch nicht direkt verfügbar. Sentimentanalyse wird deaktiviert.\")\n",
        "    use_sentiment = False\n",
        "\n",
        "config = aai.TranscriptionConfig(\n",
        "    speech_model=aai.SpeechModel.best,\n",
        "    speaker_labels=True,\n",
        "    speakers_expected = speakers_expected,\n",
        "    language_code=language_code,\n",
        "    sentiment_analysis=use_sentiment\n",
        "    )\n",
        "\n",
        "print(\"Starte Transkription...\")\n",
        "transcriber = aai.Transcriber(config=config)\n",
        "transcript = transcriber.transcribe(audio_file)\n",
        "\n",
        "\n",
        "# Speichern des Transkripts\n",
        "if transcript.status == aai.TranscriptStatus.error:\n",
        "    print(f\"Fehler bei der Transkription: {transcript.error}\")\n",
        "else:\n",
        "    transcript_file = audio_file.replace(\".mp3\", \"_transkript.md\").replace(\".wav\", \"_transkript.md\").replace(\".w4a\", \"_transkript.md\").replace(\".mp4\", \"_transkript.md\")\n",
        "    print(f\"Speichere Transkription als {transcript_file}...\")\n",
        "    with open(transcript_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Transkriptionsergebnis mit Sprecherlabels\\n\\n\")\n",
        "        for utterance in transcript.utterances:\n",
        "             sentiment_text = f\" (Sentiment: {utterance.sentiment})\" if use_sentiment else \"\"\n",
        "             f.write(f\"[{utterance.start/1000:.2f}-{utterance.end/1000:.2f}] Sprecher {utterance.speaker}: {utterance.text}{sentiment_text}\\n\")\n",
        "    print(f\"Transkription abgeschlossen! Datei gespeichert als {transcript_file}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qygu8Sw4ZvEl"
      },
      "source": [
        "## Erstellung eines Meeting-Protokolls mit LLM\n",
        "Das transkribierte Ergebnis wird mithilfe der OpenAI API analysiert und in ein strukturiertes Protokoll umgewandelt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbOScccRoaxU"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "from openai import OpenAI\n",
        "from google.colab import files\n",
        "import glob\n",
        "import os\n",
        "import markdown\n",
        "import random\n",
        "import string\n",
        "from datetime import datetime\n",
        "import re\n",
        "from google.colab import userdata\n",
        "import tiktoken\n",
        "import docx\n",
        "import subprocess\n",
        "import tempfile\n",
        "from io import StringIO\n",
        "import requests\n",
        "\n",
        "# === API-Auswahl und -Konfiguration ===\n",
        "while True:\n",
        "    api_choice = input(\"Wähle die API:\\n1: OpenAI\\n2: IONOS\\n\")\n",
        "    if api_choice in (\"1\", \"2\"):\n",
        "        break\n",
        "    else:\n",
        "        print(\"Ungültige Eingabe. Bitte 1 oder 2 eingeben.\")\n",
        "\n",
        "if api_choice == \"1\":\n",
        "    openai_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    if not openai_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY nicht in den geheimen Nutzerdaten gefunden.\")\n",
        "    client = OpenAI(api_key=openai_key)\n",
        "    base_url = \"https://api.openai.com/v1\"\n",
        "else:\n",
        "    ionos_token = userdata.get(\"IONOS_API_TOKEN\")\n",
        "    if not ionos_token:\n",
        "        raise ValueError(\"IONOS_API_TOKEN nicht in den geheimen Nutzerdaten gefunden.\")\n",
        "    client = OpenAI(api_key=ionos_token, base_url=\"https://openai.inference.de-txl.ionos.com/v1\")\n",
        "\n",
        "# === Verfügbare IONOS-Modelle abrufen und filtern (nur Textgenerierung) ===\n",
        "available_ionos_models = {}\n",
        "if api_choice == \"2\":\n",
        "    endpoint = \"https://openai.inference.de-txl.ionos.com/v1/models\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {ionos_token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    try:\n",
        "        ionos_response = requests.get(endpoint, headers=headers).json()\n",
        "        for model in ionos_response.get(\"data\", []):\n",
        "            model_id = model.get(\"id\")\n",
        "            # Modelle ausschließen, die Bilder generieren oder nur Embeddings liefern\n",
        "            if \"stable-diffusion\" in model_id.lower() or \"sentence-transformers\" in model_id.lower():\n",
        "                continue\n",
        "            max_tokens = model.get(\"max_tokens\")\n",
        "            if not max_tokens:\n",
        "                if \"70B\" in model_id:\n",
        "                    max_tokens = 8192\n",
        "                elif \"405B\" in model_id:\n",
        "                    max_tokens = 16384\n",
        "                elif \"8B\" in model_id:\n",
        "                    max_tokens = 4096\n",
        "                else:\n",
        "                    max_tokens = 4096\n",
        "            description = model.get(\"description\", model_id)\n",
        "            available_ionos_models[model_id] = {\"name\": model_id, \"desc\": description, \"max_tokens\": max_tokens}\n",
        "    except Exception as e:\n",
        "        print(\"Fehler beim Abruf der IONOS-Modelle, verwende Standardwerte.\")\n",
        "        available_ionos_models = {\n",
        "            \"meta-llama/CodeLlama-13b-Instruct-hf\": {\"name\": \"meta-llama/CodeLlama-13b-Instruct-hf\", \"desc\": \"CodeLlama 13B Instruct\", \"max_tokens\": 4096},\n",
        "            \"mistralai/Mistral-7B-Instruct-v0.3\": {\"name\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"desc\": \"Mistral 7B Instruct\", \"max_tokens\": 4096},\n",
        "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\"name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"desc\": \"Meta-Llama 3.1 8B Instruct\", \"max_tokens\": 4096},\n",
        "            \"meta-llama/Meta-Llama-3.1-70B-Instruct\": {\"name\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"desc\": \"Meta-Llama 3.1 70B Instruct\", \"max_tokens\": 8192},\n",
        "            \"mistralai/Mixtral-8x7B-Instruct-v0.1\": {\"name\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"desc\": \"Mixtral 8x7B Instruct\", \"max_tokens\": 4096},\n",
        "            \"BAAI/bge-m3\": {\"name\": \"BAAI/bge-m3\", \"desc\": \"BAAI bge-m3\", \"max_tokens\": 4096},\n",
        "            \"BAAI/bge-large-en-v1.5\": {\"name\": \"BAAI/bge-large-en-v1.5\", \"desc\": \"BAAI bge-large-en-v1.5\", \"max_tokens\": 4096},\n",
        "            \"openGPT-X/Teuken-7B-instruct-commercial\": {\"name\": \"openGPT-X/Teuken-7B-instruct-commercial\", \"desc\": \"openGPT-X Teuken 7B instruct commercial\", \"max_tokens\": 4096},\n",
        "            \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\": {\"name\": \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\", \"desc\": \"Meta-Llama 3.1 405B Instruct FP8\", \"max_tokens\": 16384},\n",
        "            \"meta-llama/Llama-3.3-70B-Instruct\": {\"name\": \"meta-llama/Llama-3.3-70B-Instruct\", \"desc\": \"Llama 3.3 70B Instruct\", \"max_tokens\": 8192}\n",
        "        }\n",
        "    # Kurze, leserliche Darstellung\n",
        "    short_taglines = {\n",
        "        \"meta-llama/CodeLlama-13b-Instruct-hf\": \"CodeLlama-13b-Instruct-hf | Max Tokens: 4096 | Ideal für Code & Instruct\",\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\": \"Mistral-7B-Instruct-v0.3 | Max Tokens: 4096 | Schnell & ausgewogen\",\n",
        "        \"meta-llama/Meta-Llama-3.1-8B-Instruct\": \"Meta-Llama-3.1-8B-Instruct | Max Tokens: 4096 | Kompakt & präzise\",\n",
        "        \"meta-llama/Meta-Llama-3.1-70B-Instruct\": \"Meta-Llama-3.1-70B-Instruct | Max Tokens: 8192 | Für lange Texte\",\n",
        "        \"mistralai/Mixtral-8x7B-Instruct-v0.1\": \"Mixtral-8x7B-Instruct-v0.1 | Max Tokens: 4096 | Multilingual\",\n",
        "        \"BAAI/bge-m3\": \"BAAI/bge-m3 | Max Tokens: 4096 | Effizient & wirtschaftlich\",\n",
        "        \"BAAI/bge-large-en-v1.5\": \"BAAI/bge-large-en-v1.5 | Max Tokens: 4096 | Stark in Englisch\",\n",
        "        \"openGPT-X/Teuken-7B-instruct-commercial\": \"openGPT-X/Teuken-7B-instruct-commercial | Max Tokens: 4096 | Kommerziell\",\n",
        "        \"meta-llama/Meta-Llama-3.1-405B-Instruct-FP8\": \"Meta-Llama-3.1-405B-Instruct-FP8 | Max Tokens: 16384 | Großer Kontext\",\n",
        "        \"meta-llama/Llama-3.3-70B-Instruct\": \"Llama-3.3-70B-Instruct | Max Tokens: 8192 | Modern & dialogstark\"\n",
        "    }\n",
        "    filtered_models = {}\n",
        "    count = 1\n",
        "    for model_id, info in available_ionos_models.items():\n",
        "        if model_id in short_taglines:\n",
        "            filtered_models[str(count)] = {\"name\": model_id, \"short\": short_taglines[model_id], \"max_tokens\": info[\"max_tokens\"]}\n",
        "            count += 1\n",
        "\n",
        "# === Transkriptdatei suchen und laden ===\n",
        "transcript_files = []\n",
        "for ext in [\"srt\", \"txt\", \"docx\", \"pdf\", \"md\"]:\n",
        "    transcript_files.extend(glob.glob(f\"/content/*_transkript.{ext}\"))\n",
        "if not transcript_files:\n",
        "    raise FileNotFoundError(\"Keine Transkriptdatei gefunden.\")\n",
        "transcript_file = transcript_files[0]\n",
        "print(f\"Verwendete Transkriptdatei: {transcript_file}\")\n",
        "\n",
        "def read_transcript_content(transcript_file):\n",
        "    if transcript_file.endswith((\".txt\", \".md\", \".srt\")):\n",
        "        with open(transcript_file, \"r\", encoding=\"utf-8\") as file:\n",
        "            return file.read()\n",
        "    elif transcript_file.endswith(\".docx\"):\n",
        "        doc = docx.Document(transcript_file)\n",
        "        return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    elif transcript_file.endswith(\".pdf\"):\n",
        "        try:\n",
        "            temp_file = tempfile.NamedTemporaryFile(suffix=\".txt\", delete=False)\n",
        "            subprocess.run([\"pdftotext\", transcript_file, temp_file.name], check=True)\n",
        "            with open(temp_file.name, \"r\", encoding=\"utf-8\") as file:\n",
        "                content = file.read()\n",
        "            os.unlink(temp_file.name)\n",
        "            return content\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Fehler beim Lesen der PDF-Datei: {e}\")\n",
        "\n",
        "transcript_content = read_transcript_content(transcript_file)\n",
        "\n",
        "# === CSS-Vorlage für HTML-Ausgabe ===\n",
        "HTML_STYLES = \"\"\"\n",
        "<style>\n",
        "body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; }\n",
        "h1, h2, h3, h4, h5, h6 { color: #333; }\n",
        "code { background-color: #f4f4f4; padding: 2px 4px; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# === Modusauswahl ===\n",
        "while True:\n",
        "    try:\n",
        "        mode = int(input(\"Wähle den Modus:\\n1: Standard-Zusammenfassung\\n2: Individuelle Aufgabe\\n3: Ultimatives Schema\\n4: Besprechungsprotokoll\\n5: Auf eigene Weise\\n\"))\n",
        "        if mode in (1, 2, 3, 4, 5):\n",
        "            break\n",
        "        else:\n",
        "            print(\"Ungültige Eingabe. Bitte 1, 2, 3, 4 oder 5 eingeben.\")\n",
        "    except ValueError:\n",
        "        print(\"Ungültige Eingabe. Bitte eine Zahl eingeben.\")\n",
        "\n",
        "# === Modellauswahl ===\n",
        "if api_choice == \"1\":\n",
        "    openai_models = {\n",
        "        \"1\": \"gpt-4o\",\n",
        "        \"2\": \"o1\",\n",
        "        \"3\": \"o3-mini\"\n",
        "    }\n",
        "    model_prompt = \"Wähle ein OpenAI-Modell:\\n\"\n",
        "    for key in openai_models:\n",
        "        model_prompt += f\"{key}: {openai_models[key]}\\n\"\n",
        "    model_prompt += \"4: Benutzerdefinierte Eingabe\\n\"\n",
        "else:\n",
        "    model_prompt = \"Verfügbare IONOS-Modelle (nur Textgenerierung):\\n\"\n",
        "    for num, model_info in filtered_models.items():\n",
        "        model_prompt += f\"{num}: {model_info['short']}\\n\"\n",
        "    model_prompt += \"8: Benutzerdefinierte Eingabe\\n\"\n",
        "\n",
        "while True:\n",
        "    model_choice = input(model_prompt)\n",
        "    if api_choice == \"1\" and model_choice in openai_models:\n",
        "        break\n",
        "    elif api_choice == \"2\" and (model_choice in filtered_models or model_choice in (\"8\",)):\n",
        "        break\n",
        "    print(\"Ungültige Eingabe. Bitte eine gültige Option wählen.\")\n",
        "\n",
        "if api_choice == \"1\":\n",
        "    if model_choice == \"4\":\n",
        "        selected_model = input(\"Gib den benutzerdefinierten Modellnamen ein: \")\n",
        "    else:\n",
        "        selected_model = openai_models[model_choice]\n",
        "else:\n",
        "    if model_choice == \"8\":\n",
        "        selected_model = input(\"Gib den benutzerdefinierten Modellnamen ein: \")\n",
        "    else:\n",
        "        selected_model = filtered_models[model_choice][\"name\"]\n",
        "\n",
        "# === Hilfsfunktionen für Schlüsselwörter und Dateinamen ===\n",
        "def extract_keyword(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if len(word) > 3 and word.isalpha():\n",
        "            word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    if word_freq:\n",
        "        return max(word_freq, key=word_freq.get)\n",
        "    return \"Thema\"\n",
        "\n",
        "def generate_creative_keyword(keyword, model_name):\n",
        "    adjectives = [\n",
        "        \"strahlend\", \"nebulös\", \"tiefgründig\", \"kristallklar\", \"abenteuerlich\",\n",
        "        \"geheimnisvoll\", \"dynamisch\", \"phantastisch\", \"wunderbar\", \"einzigartig\",\n",
        "        \"lebendig\", \"verspielt\", \"episch\", \"harmonisch\", \"unendlich\"\n",
        "    ]\n",
        "    random_adjective = random.choice(adjectives)\n",
        "    return f\"{random_adjective}-{model_name.replace('/', '-')}-{keyword}\".replace(\" \", \"-\").lower()\n",
        "\n",
        "def merge_protocol_parts(protocol_parts, model, selected_model):\n",
        "    separator = \"\\n\\n\"\n",
        "    merge_prompt = f\"\"\"Fasse die folgenden Protokollteile zu einem kohärenten, zusammenhängenden Besprechungsprotokoll zusammen.\n",
        "Achte darauf, dass alle wesentlichen Informationen, Entscheidungen, offenen Fragen, Lessons Learned, Metaphern und\n",
        "Agenda-Punkte (für den nächsten Termin) klar strukturiert und übersichtlich dargestellt werden.\n",
        "\n",
        "Protokollteile:\n",
        "{separator.join(protocol_parts)}\n",
        "\n",
        "Erstelle daraus ein einheitliches, gut strukturiertes Protokoll:\"\"\"\n",
        "    merge_messages = [{\"role\": \"user\", \"content\": merge_prompt}]\n",
        "    response = model.chat.completions.create(\n",
        "        model=selected_model,\n",
        "        messages=merge_messages\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def split_transcript(transcript, max_tokens, model_name):\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model_name)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(transcript)\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return [transcript]\n",
        "    parts = []\n",
        "    current_part = \"\"\n",
        "    current_tokens = 0\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', transcript)\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = len(encoding.encode(sentence))\n",
        "        if current_tokens + sentence_tokens <= max_tokens:\n",
        "            current_part += sentence + \" \"\n",
        "            current_tokens += sentence_tokens\n",
        "        else:\n",
        "            parts.append(current_part.strip())\n",
        "            current_part = sentence + \" \"\n",
        "            current_tokens = sentence_tokens\n",
        "    if current_part:\n",
        "        parts.append(current_part.strip())\n",
        "    return parts\n",
        "\n",
        "# === Prompt-Konfiguration für die verschiedenen Modi ===\n",
        "if mode == 1:\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Erstelle eine prägnante, übersichtliche Meeting-Zusammenfassung.\n",
        "- Extrahiere die wichtigsten Punkte und Erkenntnisse.\n",
        "- Formatiere klar strukturiert im Markdown-Format (klare Überschriften, Absätze, Bulletpoints).\n",
        "- Nutze die maximal verfügbare Tokenanzahl.\n",
        "\n",
        "Transkript:\n",
        "{transcript_content}\"\"\"\n",
        "    }]\n",
        "elif mode == 2:\n",
        "    user_prompt = input(\"Gib deine individuelle Aufgabe ein:\\n\")\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"{transcript_content}\\n\\n{user_prompt}\\n\\nBitte strukturiere die Antwort übersichtlich im Markdown-Format.\"\n",
        "    }]\n",
        "elif mode == 3:\n",
        "    schema_prompt = \"\"\"\n",
        "Erstelle eine umfassende Meeting-Zusammenfassung mit folgender Struktur:\n",
        "1. **Grundlegende Metadaten** (Datum, Uhrzeit, Ort, Teilnehmer)\n",
        "2. **Diskursanalyse** (Sprecher-Dominanz, Emotionen, Intensität der Diskussion)\n",
        "3. **Entscheidungen & Aufgaben** (mit Begründung, nur belegbare To-Dos, und zusätzliche, optionale Aufgaben als separater Abschnitt)\n",
        "4. **Follow-up** (Lessons Learned, offene Fragen, Agenda für den nächsten Termin)\n",
        "Formatiere die Antwort klar strukturiert im Markdown-Format (Überschriften, Absätze, Bulletpoints).\n",
        "    \"\"\"\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Transkript:\\n{transcript_content}\\n\\n{schema_prompt}\"\n",
        "    }]\n",
        "elif mode == 4:\n",
        "    # Punkt 4: Besprechungsprotokoll\n",
        "    protocol_template = \"\"\"\n",
        "Erstelle ein detailiertes Besprechungsprotokoll im Markdown-Format, das folgende Abschnitte beinhaltet:\n",
        "\n",
        "1. **Kopfzeile**\n",
        "   - Datum: [TT.MM.JJJJ]\n",
        "   - Uhrzeit: [HH:MM]\n",
        "   - Ort: [Besprechungsraum]\n",
        "   - Teilnehmer: [Liste]\n",
        "\n",
        "2. **Agenda**\n",
        "   - Kurze Übersicht der besprochenen Themen\n",
        "   - Auflistung der einzelnen Punkte\n",
        "\n",
        "3. **Diskussionspunkte**\n",
        "   - Detaillierte, strukturierte Zusammenfassung der Diskussion\n",
        "   - Wichtige Zitate (mit Quellenangabe, z.B. „... Zitat ...“)\n",
        "\n",
        "4. **Entscheidungen & Aufgaben**\n",
        "   - Übersicht aller getroffenen Entscheidungen und eindeutig belegbarer To-Dos\n",
        "   - Separater Abschnitt für *Mögliche von <Modell> erarbeitete Todos* (für weitergehende Ideen)\n",
        "\n",
        "5. **Offene Fragen / Weiteres**\n",
        "   - Liste offener Themen für zukünftige Besprechungen\n",
        "\n",
        "6. **Lessons Learned**\n",
        "   - Zentrale Erkenntnisse und kritische Einsichten\n",
        "\n",
        "7. **Stärkste Metaphern / Schlüssel-Aussagen**\n",
        "   - Die einprägsamsten Formulierungen und Metaphern\n",
        "\n",
        "8. **Erkenntnisse & Unvollständige Ideen**\n",
        "   - Zusätzliche Einsichten, die noch nicht abschließend bewertet wurden\n",
        "\n",
        "9. **Agenda für den nächsten Termin**\n",
        "   - Vorläufige Punkte basierend auf offenen Fragen\n",
        "\n",
        "Bitte formatiere alle Abschnitte übersichtlich, mit klaren, fettgedruckten Überschriften und strukturierten Absätzen.\n",
        "\n",
        "Transkript:\n",
        "{TRANSCRIPT}\n",
        "\"\"\"\n",
        "    prompt_template = protocol_template\n",
        "    if api_choice == \"2\" and model_choice not in (\"4\", \"8\"):\n",
        "        transcript_parts = split_transcript(transcript_content, filtered_models[model_choice][\"max_tokens\"], selected_model)\n",
        "    else:\n",
        "        transcript_parts = [transcript_content]\n",
        "    protocol_parts = []\n",
        "    for i, part in enumerate(transcript_parts):\n",
        "        current_prompt = prompt_template.replace(\"{TRANSCRIPT}\", part)\n",
        "        messages = [{\"role\": \"user\", \"content\": current_prompt}]\n",
        "        print(f\"Sende Teil {i+1} von {len(transcript_parts)}...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=selected_model if api_choice == \"1\" else filtered_models[model_choice][\"name\"],\n",
        "            messages=messages\n",
        "        )\n",
        "        protocol_parts.append(response.choices[0].message.content.strip())\n",
        "    if len(protocol_parts) > 1:\n",
        "        protocol = merge_protocol_parts(protocol_parts, client, selected_model)\n",
        "    else:\n",
        "        protocol = protocol_parts[0]\n",
        "elif mode == 5:\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Analysiere das folgende Transkript eines Meetings und erstelle ein übersichtliches Protokoll im Markdown-Format.\n",
        "Berücksichtige:\n",
        "- Eindeutige Extraktion der Teilnehmer (Name, Position)\n",
        "- Strukturierte Darstellung der Inhalte (klare Überschriften, Absätze und Bulletpoints)\n",
        "- Zusammenfassung der Kernpunkte, Entscheidungen, offenen Fragen, Lessons Learned und Vorschläge für die Agenda des nächsten Treffens\n",
        "\n",
        "Transkript:\n",
        "{transcript_content}\"\"\"\n",
        "    }]\n",
        "\n",
        "# === API-Anfrage und Antwortverarbeitung ===\n",
        "if api_choice == \"1\":\n",
        "    response = client.chat.completions.create(\n",
        "        model=selected_model,\n",
        "        messages=messages\n",
        "    )\n",
        "    protocol = response.choices[0].message.content.strip()\n",
        "else:\n",
        "    if model_choice == \"8\":\n",
        "        print(\"Für benutzerdefinierte Modelle wird das Transkript nicht aufgeteilt.\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=selected_model,\n",
        "            messages=messages\n",
        "        )\n",
        "        protocol = response.choices[0].message.content.strip()\n",
        "    else:\n",
        "        if mode != 4:\n",
        "            selected_model_data = filtered_models[model_choice]\n",
        "            max_tokens = selected_model_data[\"max_tokens\"]\n",
        "            transcript_parts = split_transcript(transcript_content, max_tokens, selected_model_data[\"name\"])\n",
        "            protocol_parts = []\n",
        "            for i, part in enumerate(transcript_parts):\n",
        "                print(f\"Sende Teil {i+1} von {len(transcript_parts)}...\")\n",
        "                messages[0][\"content\"] = messages[0][\"content\"].replace(transcript_content, part)\n",
        "                response = client.chat.completions.create(\n",
        "                    model=selected_model_data[\"name\"],\n",
        "                    messages=messages\n",
        "                )\n",
        "                protocol_parts.append(response.choices[0].message.content.strip())\n",
        "            protocol = \"\\n\\n\".join(protocol_parts)\n",
        "\n",
        "# === Zusätzlichen Header einfügen: Modell & Prompt-Option ===\n",
        "prompt_option_names = {\n",
        "    1: \"Standard-Zusammenfassung\",\n",
        "    2: \"Individuelle Aufgabe\",\n",
        "    3: \"Ultimatives Schema\",\n",
        "    4: \"Besprechungsprotokoll\",\n",
        "    5: \"Auf eigene Weise\"\n",
        "}\n",
        "header_line = f\"*Erstellt mit Modell: {selected_model} | Prompt: {prompt_option_names[mode]}*\"\n",
        "protocol = header_line + \"\\n\\n\" + protocol\n",
        "\n",
        "# === Dateinamen generieren basierend auf einem vom Modell erarbeiteten Schlagwort ===\n",
        "name_prompt = f\"Fasse das folgende Protokoll in einem prägnanten, einprägsamen Schlagwort zusammen, das als Dateiname verwendet werden kann. Gib nur das Schlagwort zurück:\\n\\n{protocol}\"\n",
        "name_messages = [{\"role\": \"user\", \"content\": name_prompt}]\n",
        "name_response = client.chat.completions.create(\n",
        "    model=selected_model if api_choice == \"1\" else (selected_model if model_choice == \"8\" else filtered_models[model_choice][\"name\"]),\n",
        "    messages=name_messages\n",
        ")\n",
        "filename_keyword = name_response.choices[0].message.content.strip().replace(\" \", \"-\")\n",
        "\n",
        "datum = datetime.now().strftime(\"%d.%m\")\n",
        "random_string = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "md_filename = f\"{filename_keyword}.{datum}#{mode}.{random_string}.md\"\n",
        "html_filename = f\"{filename_keyword}.{datum}#{mode}.{random_string}.html\"\n",
        "\n",
        "output_dir = \"/content/Transkripte\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def create_html(content):\n",
        "    html_content = markdown.markdown(content, extensions=['extra'])\n",
        "    return f\"{HTML_STYLES}\\n{html_content}\"\n",
        "\n",
        "with open(os.path.join(output_dir, md_filename), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(protocol)\n",
        "with open(os.path.join(output_dir, html_filename), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(create_html(protocol))\n",
        "\n",
        "print(f\"\\nErgebnisse gespeichert als:\")\n",
        "print(f\"- {os.path.join(output_dir, md_filename)}\")\n",
        "print(f\"- {os.path.join(output_dir, html_filename)}\")\n",
        "files.download(os.path.join(output_dir, md_filename))\n",
        "files.download(os.path.join(output_dir, html_filename))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}